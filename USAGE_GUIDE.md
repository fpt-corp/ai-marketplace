# Token Calculator â€” Usage Guide

_CÃ´ng cá»¥ tÃ­nh token cho LLM vÃ  Embedding Models_

---

## ğŸ“š Má»¥c lá»¥c

1. **[Tá»•ng quan](#tá»•ng-quan)**
2. **[CÃ i Ä‘áº·t](#cÃ i-Ä‘áº·t)**
3. **[LLM Token Calculator](#llm-token-calculator)**
   - [CÃ¡ch Ä‘áº¿m Input Tokens (HuggingFace Tokenizer)](#cÃ¡ch-Ä‘áº¿m-input-tokens-theo-huggingface-tokenizer)
4. **[Embedding Token Calculator](#embedding-token-calculator)**
   - [CÃ¡ch Ä‘áº¿m Input Tokens (HuggingFace Tokenizer)](#cÃ¡ch-Ä‘áº¿m-input-tokens-theo-huggingface-tokenizer-1)
5. **[Reference](#reference)**

---

## Tá»•ng quan

Bá»™ cÃ´ng cá»¥ gá»“m 2 calculator chuyÃªn biá»‡t:

### 1. LLM Token Calculator (`complete_token_calculator_llm.py`)

DÃ nh cho **Chat Completion models** (Gemma, GPT, Claude, v.v.)

**So sÃ¡nh 2 phÆ°Æ¡ng phÃ¡p:**
- âœ… **OpenAI Client** (baseline - ground truth)
- âœ… **LiteLLM** (wrapper library)

**Äáº·c Ä‘iá»ƒm:**
- Äáº¿m cáº£ `prompt_tokens` vÃ  `completion_tokens`
- Input: Messages (chat format)
- Output: Response text + token usage

### 2. Embedding Token Calculator (`complete_token_calculator_embedding.py`)

DÃ nh cho **Embedding models** (E5, text-embedding-ada-002, v.v.)

**So sÃ¡nh 3 phÆ°Æ¡ng phÃ¡p:**
- âœ… **OpenAI Client** (baseline - ground truth)
- âœ… **LiteLLM** (wrapper library)
- âœ… **LangChain** (high-level framework)

**Äáº·c Ä‘iá»ƒm:**
- Chá»‰ Ä‘áº¿m `prompt_tokens` (embeddings khÃ´ng cÃ³ completion)
- Input: Text hoáº·c list of texts
- Output: Embedding vectors + token usage

---

## CÃ i Ä‘áº·t

### YÃªu cáº§u

```bash
Python >= 3.8
```

### Dependencies

```bash
# Core libraries
pip install litellm openai

# Optional: LangChain (cho embedding calculator)
pip install langchain-openai

# Optional: Custom tokenizer support
pip install transformers torch
```

---

## LLM Token Calculator

### CÃ¡ch cháº¡y

```bash
python complete_token_calculator_llm.py
```

### Sá»­ dá»¥ng trong code

```python
from complete_token_calculator_llm import TokenCalculator

# Khá»Ÿi táº¡o
calc = TokenCalculator(
    api_base="https://mkp-api.fptcloud.com",
    api_key="sk-your-api-key",
    model="openai/gemma-3-27b-it"
)

# Messages input
messages = [
    {"role": "user", "content": "Hello, how are you?"}
]

# Method 1: OpenAI Client (baseline)
openai_result = calc.calculate_tokens_openai(messages)
print(f"OpenAI: {openai_result.total_tokens} tokens")

# Method 2: LiteLLM
litellm_result = calc.calculate_tokens(messages)
print(f"LiteLLM: {litellm_result.total_tokens} tokens")
```

### Output máº«u

```
================================================================================
TOKEN CALCULATOR - LiteLLM with Custom Tokenizer
================================================================================

Response: 'Hello! How can I help you today?'

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Method                  â”‚    Prompt     â”‚   Completion    â”‚      Total      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ MKP API (Production)    â”‚      18       â”‚       10        â”‚       28        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ LiteLLM                 â”‚      18       â”‚       10        â”‚       28        â”‚ âœ“
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### CÆ¡ cháº¿ hoáº¡t Ä‘á»™ng

#### Method 1: OpenAI Client (Baseline)

```
User â†’ messages â†’ OpenAI Client â†’ API Call â†’ Response
                                                 â†“
                                            usage.prompt_tokens
                                            usage.completion_tokens
                                            usage.total_tokens
```

```python
def calculate_tokens_openai(self, messages, **kwargs):
    # Gá»i API qua OpenAI client
    client = OpenAI(api_key=self.api_key, base_url=self.api_base)
    response = client.chat.completions.create(
        model=self.model,
        messages=messages,
        **kwargs
    )

    # Láº¥y token usage tá»« response
    usage = response.usage
    return UsageResult(
        prompt_tokens=usage.prompt_tokens,
        completion_tokens=usage.completion_tokens,
        total_tokens=usage.total_tokens,
        ...
    )
```

#### Method 2: LiteLLM

```
User â†’ messages â†’ LiteLLM â†’ API Call â†’ Response
                                          â†“
                                    usage.prompt_tokens
                                    usage.completion_tokens
                                    usage.total_tokens
```

```python
def calculate_tokens(self, messages, **kwargs):
    # Gá»i API qua LiteLLM
    response = litellm.completion(
        model=self.model,
        messages=messages,
        api_base=self.api_base,
        api_key=self.api_key,
        **kwargs
    )

    # Láº¥y token usage tá»« response
    usage = response.usage
    return UsageResult(
        prompt_tokens=usage.prompt_tokens,
        completion_tokens=usage.completion_tokens,
        total_tokens=usage.total_tokens,
        ...
    )
```

**Káº¿t luáº­n**: Cáº£ 2 method Ä‘á»u gá»i API thá»±c vÃ  Ä‘á»u cho káº¿t quáº£ chÃ­nh xÃ¡c nhÆ° nhau. LiteLLM lÃ  wrapper giÃºp code Ä‘Æ¡n giáº£n hÆ¡n khi lÃ m viá»‡c vá»›i nhiá»u provider.

### CÃ¡ch Ä‘áº¿m Input Tokens theo HuggingFace Tokenizer

CÃ¡c API methods á»Ÿ trÃªn tráº£ vá» sá»‘ tokens tá»« server. Äá»ƒ hiá»ƒu rÃµ cÃ¡ch server Ä‘áº¿m tokens, ta cáº§n biáº¿t tokenizer mÃ  model sá»­ dá»¥ng.

#### Model: google/gemma-3-27b-it

**Tokenizer**: Gemma sá»­ dá»¥ng **Gemma tokenizer** (dá»±a trÃªn SentencePiece)

```python
from transformers import AutoTokenizer

# Load tokenizer tá»« HuggingFace
tokenizer = AutoTokenizer.from_pretrained("google/gemma-2-27b-it")

# Äáº¿m tokens cho input
messages = [{"role": "user", "content": "Hello, how are you?"}]

# Apply chat template
formatted_prompt = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True
)

# Tokenize vÃ  Ä‘áº¿m
tokens = tokenizer.encode(formatted_prompt)
num_tokens = len(tokens)

print(f"Input text: {formatted_prompt}")
print(f"Number of tokens: {num_tokens}")
print(f"Token IDs: {tokens}")
```

**CÆ¡ cháº¿ Ä‘áº¿m:**
1. Chat messages Ä‘Æ°á»£c format theo template cá»§a model (beg_of_turn, end_of_turn tokens)
2. Text Ä‘Æ°á»£c tokenize thÃ nh subword pieces
3. Special tokens Ä‘Æ°á»£c thÃªm vÃ o: `<bos>`, `<eos>`, `<start_of_turn>`, `<end_of_turn>`

**VÃ­ dá»¥ vá»›i "Hello, how are you?":**
```
<bos><start_of_turn>user
Hello, how are you?<end_of_turn>
<start_of_turn>model

Tokens: [2, 106, 1645, 108, 4521, 235269, 1368, 708, 692, 235336, 107, 108, 106, 2516, 108]
Count: 15 tokens (bao gá»“m special tokens)
```

**LÆ°u Ã½:**
- API cÃ³ thá»ƒ tráº£ vá» sá»‘ tokens khÃ¡c má»™t chÃºt do cÃ¡ch xá»­ lÃ½ internal
- Special tokens vÃ  chat template format áº£nh hÆ°á»Ÿng Ä‘áº¿n token count
- Gemma-2-27b-it vÃ  Gemma-3-27b-it cÃ³ thá»ƒ dÃ¹ng cÃ¹ng tokenizer

---

## Embedding Token Calculator

### CÃ¡ch cháº¡y

```bash
python complete_token_calculator_embedding.py
```

### Sá»­ dá»¥ng trong code

```python
from complete_token_calculator_embedding import EmbeddingTokenCalculator

# Khá»Ÿi táº¡o
calc = EmbeddingTokenCalculator(
    api_base="https://mkp-api.fptcloud.com",
    api_key="sk-your-api-key",
    model="multilingual-e5-large"
)

# Text input
text = "Hello, how are you?"

# Method 1: OpenAI Client (baseline)
openai_result = calc.calculate_litellm_tokens_openai(text)
print(f"OpenAI: {openai_result.total_tokens} tokens")

# Method 2: LiteLLM
litellm_result = calc.calculate_litellm_tokens(text)
print(f"LiteLLM: {litellm_result.total_tokens} tokens")

# Method 3: LangChain (optional)
langchain_result = calc.calculate_litellm_tokens_langchain(text)
print(f"LangChain: {langchain_result.total_tokens} tokens")
```

### Output máº«u

```
==================================================================================
EMBEDDING TOKEN CALCULATOR - intfloat/multilingual-e5-large
Comparison: OpenAI Client vs LiteLLM vs LangChain
==================================================================================

Input: 'Hello, how are you?'

Testing 3 methods...

1ï¸âƒ£  OpenAI Client (Baseline - Ground Truth)
   Result: 19 tokens
   Response time: 371.82ms

2ï¸âƒ£  LiteLLM API
   Result: 19 tokens
   Response time: 226.84ms

3ï¸âƒ£  LangChain OpenAIEmbeddings
   Result: 19 tokens
   Response time: 213.59ms

==================================================================================
COMPARISON TABLE
==================================================================================

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Method                       â”‚ Prompt Tokens â”‚  Total Tokens â”‚     Difference     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ OpenAI Client (Baseline)     â”‚       19      â”‚       19      â”‚         -          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ MKP API (Production)         â”‚       19      â”‚       19      â”‚         âœ“          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ LangChain OpenAIEmbeddings   â”‚       19      â”‚       19      â”‚         âœ“          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### CÆ¡ cháº¿ hoáº¡t Ä‘á»™ng

#### Method 1: OpenAI Client (Baseline)

```
User â†’ text â†’ OpenAI Client â†’ embeddings.create() â†’ Response
                                                        â†“
                                                  usage.prompt_tokens
                                                  usage.total_tokens
```

```python
def calculate_litellm_tokens_openai(self, input_texts, **kwargs):
    # Gá»i API qua OpenAI client
    client = OpenAI(api_key=self.api_key, base_url=self.api_base)
    response = client.embeddings.create(
        model=self.model,
        input=input_texts,
        **kwargs
    )

    # Láº¥y token usage tá»« response
    usage = response.usage
    return EmbeddingUsageResult(
        prompt_tokens=usage.prompt_tokens,
        total_tokens=usage.total_tokens,
        ...
    )
```

#### Method 2: LiteLLM

```
User â†’ text â†’ LiteLLM â†’ litellm.embedding() â†’ Response
                                                  â†“
                                            usage.prompt_tokens
                                            usage.total_tokens
```

```python
def calculate_litellm_tokens(self, input_texts, **kwargs):
    # Gá»i API qua LiteLLM
    response = litellm.embedding(
        model=self.model,
        input=input_texts,
        api_base=self.api_base,
        api_key=self.api_key,
        **kwargs
    )

    # Láº¥y token usage tá»« response
    usage = response.usage
    return EmbeddingUsageResult(
        prompt_tokens=usage.prompt_tokens,
        total_tokens=usage.total_tokens,
        ...
    )
```

#### Method 3: LangChain

```
User â†’ text â†’ LangChain â†’ OpenAIEmbeddings â†’ underlying client â†’ Response
                                                                     â†“
                                                               usage.prompt_tokens
                                                               usage.total_tokens
```

```python
def calculate_litellm_tokens_langchain(self, input_texts, **kwargs):
    # Táº¡o LangChain wrapper
    embeddings = OpenAIEmbeddings(
        model=self.model,
        openai_api_base=self.api_base,
        openai_api_key=self.api_key
    )

    # Truy cáº­p underlying client cá»§a LangChain
    embeddings_client = embeddings.client

    # Gá»i API trá»±c tiáº¿p Ä‘á»ƒ láº¥y usage info
    response = embeddings_client.create(
        model=self.model,
        input=texts_list,
        **kwargs
    )

    # Láº¥y token usage tá»« response
    usage = response.usage
    return EmbeddingUsageResult(
        prompt_tokens=usage.prompt_tokens,
        total_tokens=usage.total_tokens,
        ...
    )
```

**Káº¿t luáº­n**: Táº¥t cáº£ 3 methods Ä‘á»u gá»i API thá»±c vÃ  cho káº¿t quáº£ giá»‘ng nhau. Chá»n method phÃ¹ há»£p vá»›i stack cÃ´ng nghá»‡ báº¡n Ä‘ang dÃ¹ng:
- **OpenAI Client**: DÃ¹ng khi chá»‰ cáº§n OpenAI API
- **LiteLLM**: DÃ¹ng khi cáº§n support nhiá»u providers
- **LangChain**: DÃ¹ng khi Ä‘Ã£ cÃ³ há»‡ thá»‘ng LangChain

### CÃ¡ch Ä‘áº¿m Input Tokens theo HuggingFace Tokenizer

CÃ¡c API methods á»Ÿ trÃªn tráº£ vá» sá»‘ tokens tá»« AI Serving Engine. Äá»ƒ hiá»ƒu rÃµ cÃ¡ch AI Engine Ä‘áº¿m tokens, ta cáº§n biáº¿t tokenizer mÃ  model sá»­ dá»¥ng.
- **Gemma Models**: https://huggingface.co/google/gemma-2-27b-it
- **E5 Embedding Models**: https://huggingface.co/intfloat/multilingual-e5-large

## Reference

### API & Libraries
- **LiteLLM Docs**: https://docs.litellm.ai/
- **OpenAI API**: https://platform.openai.com/docs
- **LangChain Docs**: https://python.langchain.com/docs/

### Tokenizers
- **HuggingFace Transformers**: https://huggingface.co/docs/transformers/
- **Tokenizers Guide**: https://huggingface.co/docs/transformers/tokenizer_summary

### Tokenizer Implementation
- **SentencePiece**: https://github.com/google/sentencepiece
- **BPE Algorithm**: https://en.wikipedia.org/wiki/Byte_pair_encoding
- **XLM-RoBERTa**: https://huggingface.co/xlm-roberta-large
